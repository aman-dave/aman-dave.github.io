{"componentChunkName":"component---src-templates-note-template-js","path":"/notes/Coursera: Big Data/Intro To Big Data/","webpackCompilationHash":"1307dca3626bbb061791","result":{"data":{"markdownRemark":{"html":"<h1>Big Data</h1>\n<h2>Big Data Technologies</h2>\n<ul>\n<li>Hadoop is designed to support the processing of large data sets in a distributed computing environment. Namely, the volume of unstructured information.</li>\n<li>Hadoop can handle big batches of distributed information but most often there's a need for a real time processing of people generated data like Twitter or Facebook updates.</li>\n<li>Social media and market data are two types of what we call high velocity data</li>\n<li>Storm and Spark are two other open source frameworks that handle such real time data generated at a fast rate.</li>\n<li><strong>ETL:</strong> Extract Transform Load. Fairly static.</li>\n<li><strong><em>Neo4j:</em></strong> Graph Database</li>\n<li><strong><em>Cassandra:</em></strong> Key-Value pair</li>\n<li><strong>Structured Data:</strong> Data that is stored in some relational database</li>\n</ul>\n<h2>The characteristics of Big Data</h2>\n<ul>\n<li><strong>Volume:</strong> Scale of data — <em>Size</em></li>\n<li><strong>Variety:</strong> Different forms of data — <em>Complexity</em></li>\n<li><strong>Velocity:</strong> Analysis of streaming of data — <em>Speed</em></li>\n<li><strong>Varecity:</strong> Uncertainty of data — <em>Quality</em></li>\n<li><strong>Valence:</strong> Connectedness of big data — <em>Connectedness</em></li>\n</ul>\n<p>All these 5 characterisitcs bring in <strong>VALUE</strong> to a business</p>\n<ol>\n<li>\n<p><strong>Volume:</strong> sheer size of the data</p>\n<ul>\n<li>Volume can come from large datasets being shared or many small data pieces and events being collected over time</li>\n<li>Volume is the dimension of big data related to its size and its exponential growth.</li>\n<li>\n<p>Challenges:</p>\n<ul>\n<li>Storage, networking, bandwidth, cost of storing data, in-house versus cloud storage</li>\n<li>As volume increases, performance decreases and cost increases</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Variety:</strong> additional complexity that results from different kinds of data</p>\n<ul>\n<li>\n<p>The heterogeneity of data can be characterized along four axes:</p>\n<ul>\n<li><em>Structural Variety:</em> formats and models</li>\n<li><em>Medium Variety:</em> medium in which data gets delivered</li>\n<li><em>Semantic Variety:</em> how to interpret and operate on data</li>\n<li><em>Availability Variety:</em> real-time or intermittent</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Velocity:</strong> increasing speed at which big data is created, stored and analyized</p>\n<ul>\n<li>The need for real time data-driven actions within a business case is what in the end dictates the velocity of analytics over big data.</li>\n<li>Business decisiom dictates the speed of data generation vs speed of data processing decision</li>\n</ul>\n</li>\n<li>\n<p><strong>Veracity:</strong> quality of the data—validity or volatility</p>\n<ul>\n<li><em>Accuracy of the data</em></li>\n<li><em>Reliability of the data source</em></li>\n<li><em>Context within analysis</em></li>\n<li>Unstructured data on the internet is imprecise and uncertain. In addition, high-velocity big data leaves very little or no time for ETL, and in turn, hindering the quality assurance processes of the data.</li>\n</ul>\n</li>\n<li>\n<p><strong>Valence:</strong> more the data is connected, more is its valence</p>\n<ul>\n<li>For a data collection valence measures the ratio of actually connected data items to the possible number of connections that could occur within the collection.</li>\n<li>\n<p>Challenges:</p>\n<ul>\n<li>More complex data exploration algorithms</li>\n<li>Modeling and prediction of valence changes</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2>Data Science: Getting the value of the Big Data</h2>\n<ul>\n<li>Data science can be thought of getting insights from empirical research</li>\n<li>Data Science is not just analysis of the past, but generation of actions for the future</li>\n<li>\n<p>Characteristics of a Data Scientist:</p>\n<ul>\n<li>Understand the problem they are trying to solve</li>\n<li>Aim to find the right analytical methods to arrive at a solution</li>\n</ul>\n</li>\n</ul>\n<h2>Building a Big Data Strategy</h2>\n<ul>\n<li>\n<p>Four major parts of a stratergy</p>\n<ul>\n<li><strong>Aim:</strong> It starts with big objectives and not data collection</li>\n<li><strong>Policy:</strong> Privacy concerns, Lifetime of Data, Interpolation and Regulation</li>\n<li><strong>Plan:</strong> It should be built around business objectives</li>\n<li><strong>Action:</strong> Communication, remove barriers to data access and integration</li>\n</ul>\n</li>\n</ul>\n<h2>The dimensions of Big Data</h2>\n<ul>\n<li><strong>People:</strong> Teaming up around application specific purpose</li>\n<li><strong>Purpose:</strong> Defined by Big Data strategy ideas</li>\n<li><strong>Process:</strong> The road map through which objectives are attained</li>\n<li><strong>Platforms:</strong> Technologies used to derive solutions</li>\n<li><strong>Programmability:</strong> The combination of process and platform</li>\n</ul>\n<p>All these 5 dimensions bring streamlined approach that starts and ends with the <strong>PRODUCT</strong></p>\n<ol>\n<li>\n<p><strong>People</strong></p>\n<ul>\n<li>People refers to a data science team or the projects stakeholders</li>\n</ul>\n</li>\n<li>\n<p><strong>Purpose</strong></p>\n<ul>\n<li>The purpose refers to the challenge or set of challenges defined by your big data strategy</li>\n</ul>\n</li>\n<li>\n<p><strong>Process</strong></p>\n<ul>\n<li>People with purpose will define a process to collaborate and communicate around</li>\n<li>The process is conceptual in the beginning and defines the set of steps an how everyone can contribute to it</li>\n<li>Generally a 5 step process</li>\n</ul>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 600px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/9b4ba13b612f6df6c85ef1d502eded8f/1e544/Untitled.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 23.776959495002632%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAAsSAAALEgHS3X78AAABOklEQVQY02Nwdfe3d/a2dfCwd/RycvG0tXMyMLKQlFIyNjG3dfG1dvS2cfYBIkcXF3snFwdnF3tHeysnX3UDaz4ZGwYFdVs9Yw89Ew8NfXfHwPmx+QcjM7f7Je/0syzMUtbI0TEr0NHxkvRXVtxqbLhZV2+HoeLMxcEWm2P1/PQsGfglrcUUHBQ03DSNgi18N/mnXPRJOBpf+SjabVKSoEySlFaBgkqEXAI/3wlJyYOSsmeM5Hev8TE9GasRa2jFoGZeomDWruvY7RJQp2OVp2jSoGze5eDXZuOSrWpeo2DRaejc7BXRpGBSBFRm5NZn55nPLOTNJh7MLBHG4JuwJKvtZWL1bUnNRFPnksTqq3ndb008p6ibxEcWHCzo/eQWv1lBJ8o/cVl2+8uEqmvianFsYgHc0mHcUiEA8ZxdwNp0CnQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"The 5 step process\"\n        title=\"\"\n        src=\"/static/9b4ba13b612f6df6c85ef1d502eded8f/34e8a/Untitled.png\"\n        srcset=\"/static/9b4ba13b612f6df6c85ef1d502eded8f/d4214/Untitled.png 150w,\n/static/9b4ba13b612f6df6c85ef1d502eded8f/135ae/Untitled.png 300w,\n/static/9b4ba13b612f6df6c85ef1d502eded8f/34e8a/Untitled.png 600w,\n/static/9b4ba13b612f6df6c85ef1d502eded8f/fea0e/Untitled.png 900w,\n/static/9b4ba13b612f6df6c85ef1d502eded8f/6ff5e/Untitled.png 1200w,\n/static/9b4ba13b612f6df6c85ef1d502eded8f/1e544/Untitled.png 1901w\"\n        sizes=\"(max-width: 600px) 100vw, 600px\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">The 5 step process</figcaption>\n  </figure></p>\n</li>\n<li>\n<p><strong>Platform</strong></p>\n<ul>\n<li>Big data frameworks like Hadoop</li>\n<li>Scalability is an important factor here</li>\n</ul>\n</li>\n<li>\n<p><strong>Programmability</strong></p>\n<ul>\n<li>The scalable process should be programmable through the utilization of reusable and reproducible programming interfaces to libraries, like systems middleware, analytical tools, visualization environments, and end-user reporting environments.</li>\n</ul>\n</li>\n</ol>\n<p>External Link: <a href=\"https://words.sdsc.edu/words-data-science/data-science\">https://words.sdsc.edu/words-data-science/data-science</a></p>\n<h2>Steps in Data Science Process</h2>\n<ol>\n<li><strong>Acquire Data:</strong> finding, accessing, acquiring and moving data</li>\n<li><strong>Prepare Data:</strong> explore and pre-process data</li>\n<li><strong>Analyze Data:</strong> building a model for the data for analysis</li>\n<li><strong>Communicate Results:</strong> interpret, summarize, visualize, or post process</li>\n<li><strong>Apply Results:</strong> act on the results find</li>\n</ol>\n<p>All of these are iterative steps, and one step may require the previous step to be repeated with new information.</p>\n<ol>\n<li>\n<p><strong>Acquiring Data</strong></p>\n<ul>\n<li>Obtain source material before analyzing</li>\n<li>Identify suitable data related to your problem</li>\n<li><em>REST:</em> it is an approach to implementing web services with performance, scalability and maintainability in mind</li>\n<li><em>Web Sockets:</em> allow real time modifications from web sites</li>\n<li>Traditional Database: SQL and query browsers</li>\n<li>Remote Data: Web services</li>\n<li>Text Files: Scripting languages</li>\n<li>NoSQL storage: Web services and Programming interfaces</li>\n</ul>\n</li>\n<li>\n<p><strong>Prepare Data</strong></p>\n<ol>\n<li>\n<p><strong>Exploring Data</strong></p>\n<ul>\n<li>preliminary investigation in order to gain a better understanding of the specific characteristics of your data</li>\n<li>look for things like correlations, general trends, and outliers</li>\n<li>visualize the data for better understanding</li>\n</ul>\n</li>\n<li>\n<p><strong>Pre-processing Data</strong></p>\n<ul>\n<li>clean the data to address data quality issues</li>\n<li>transform the raw data to make it suitable for analysis</li>\n<li>remove data with missing values</li>\n<li>merge duplicate records</li>\n<li>generate best estimate for invalid values</li>\n<li>remove outliers if not necessary</li>\n</ul>\n</li>\n</ol>\n</li>\n<li>\n<p><strong>Analyzing Data</strong></p>\n<ul>\n<li>Classification — predict the category of the input data</li>\n<li>Regression — predict a numeric value instead of category</li>\n<li>Clustering — organize similar items into groups</li>\n<li>Association analysis — come up with a set of rules to capture associations within items or events</li>\n<li>Graph analysis — find connection between entities</li>\n<li>Select a technique</li>\n<li>Construct the model using the data</li>\n<li>\n<p>Evaluate the model</p>\n<ul>\n<li>Classification and regression — will have the correct output for each sample in your input data</li>\n<li>Clustering — examine the groups formed</li>\n<li>Association analysis and graph analysis — investigate the results</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Reporting Insights</strong></p>\n<ul>\n<li>This step is to determine what actions should follow</li>\n<li>All findings must be presented so that informed decisions can be made</li>\n<li>Report your findings by presenting your results and value add with graphs using visualization tools</li>\n</ul>\n</li>\n<li>\n<p><strong>Turning insights into actions</strong></p>\n<ul>\n<li>Determine actions to take based on results</li>\n<li>Identify stakeholders, monitor and measure the impact and evaluate results</li>\n</ul>\n</li>\n</ol>\n<h2>Distributed File System</h2>\n<ul>\n<li><strong>Flat Structure</strong> — data units in a sequence</li>\n<li><strong>Database</strong> — hierarchy construction of index records</li>\n<li>Data replication makes the systems more fault tolerant</li>\n<li>It provides data scalability, fault tolerance, and high concurrency through partitioning and replication of data on many nodes</li>\n<li><strong>Parallel Computer</strong> is a very large number of single computing nodes with specialized capabilities connected to other networks</li>\n<li><strong>Commodity clusters</strong> are affordable parallel computers with an average number of computing nodes. The nodes in the commodity cluster are more generic in their computing capabilities.</li>\n<li>Computing in one or more of these clusters across a local area network or the internet is called distributed computing — this architecture enables <strong>data-paralleism</strong></li>\n<li>Data parallelism enables <em>scalability, performance and cost reduction</em></li>\n<li>Faults that could occur in data parallelism — connectivity of rack can stop, connections between individual nodes can break</li>\n<li><strong>Fault Tolerance</strong> — ability to recover from such faults</li>\n</ul>\n<h3>Programming Models for Big Data</h3>\n<ul>\n<li>A programming model is an abstraction or existing machinery or infrastructure</li>\n<li>It is a set of abstract runtime libraries and programming languages that form a model of computation</li>\n<li>\n<p>Requirements of big data programmin models:</p>\n<ol>\n<li>\n<p>Support Big Data Operations</p>\n<ul>\n<li>Split volumes of data</li>\n<li>Access data fast</li>\n<li>Distribute computation to nodes</li>\n</ul>\n</li>\n<li>\n<p>Handle Fault Tolerance</p>\n<ul>\n<li>Replicate Data Partitions</li>\n<li>Recover files when needed</li>\n</ul>\n</li>\n<li>Enable Adding more racks — horizontal scaling</li>\n<li>Optimized for various data types</li>\n</ol>\n</li>\n<li><strong>MapReduce</strong> is a big data programming model that supports all the requirements of big data — processing large data, split complications into different parallel tasks and make efficient use of large commodity clusters, distributed file systems, abstracts out the details of parallelization, full tolerance, data distribution, monitoring, and load balancing</li>\n</ul>\n<h1>Various frameworks and libraries used in Big Data</h1>\n<p>⇒ <strong>MapReduce</strong> is a programming model that simplifies parallel computing</p>\n<p>⇒ Hadoop is an OS implementation of MapReduce</p>\n<ul>\n<li>Provides scalability to store large volumes of data</li>\n<li>Provides fault tolerance</li>\n<li>Optimized for various data types</li>\n<li>Facilitate shared environment</li>\n<li>Provides value backed by large active community</li>\n<li>Hadoop Distrbuted File System provides scaleable and reliable storage</li>\n</ul>\n<p>⇒ <strong>Hive</strong> was created at Facebook to issue SQL-like queries using MapReduce on their data in HDFS</p>\n<p>⇒ <strong>Pig</strong> was created at Yahoo to model data flow based programs using MapReduce</p>\n<p>⇒ <strong>Giraph</strong> was built for processing large-scale graphs efficiently</p>\n<p>⇒ <strong>Storm, Spark, and Flink</strong> were built for real-time and in-memory processing of big data on top of the YARN resource scheduler and HDFS</p>\n<p>⇒ NoSQL projects like <strong>Cassandra, MongoDB, and HBase</strong> handle collections of key-values or large sparse tables</p>\n<p>⇒ <strong>Zookeeper</strong> — created by Yahoo to wrangle services named after animals</p>\n<h3>The Hadoop Distributed File System</h3>\n<ul>\n<li>It provides scalability of large data sets and reliability for managing data</li>\n<li>HDFS achieves scalability by partitioning or splitting large files across multiple computers</li>\n<li>HDFS is comprised of two components — NameNode and DataNode</li>\n<li>They have master-slave relationship</li>\n<li>NameNode — usually one per cluster and DataNode — usually one per machine</li>\n<li>NameNode acts as an administrator of HDFS</li>\n<li>When the file is created, the NameNode records the name, location in the directory hierarchy and other metadata</li>\n<li>The NameNode also decides which data nodes to store the contents of the file and remembers this mapping</li>\n<li>The DataNode runs on each node in the cluster, and is responsible for storing file blocks</li>\n</ul>\n<h3>Yarn — Resource Manager for Hadoop</h3>\n<ul>\n<li>YARN is a resource manage layer that sits just above the storage layer HDFS</li>\n<li>The <strong>resource manager</strong> controls all the resources, and decides who gets what</li>\n<li><strong>Node manager</strong> operates at machine level and is in charge of a single machine</li>\n<li>Each application gets an <strong>application master —</strong> It negotiates resource from the <strong>Resource Manager</strong> and it talks to Node Manager to get its tasks completed</li>\n<li>The <strong>container</strong> is an abstract notion that signifies a resource — i.e. a collection of CPU memory, disk, network, and other resources</li>\n</ul>\n<h3>MapReduce</h3>\n<ul>\n<li><strong>Map</strong> and <strong>Reduce</strong> are two concepts based on functional programming where the output the function is based solely on the input</li>\n<li>For map, the operation is applied on each data element</li>\n<li>In reduce, the operation summarizes elements in some manner</li>\n<li>MapReduce excels at independent batch tasks</li>\n<li>MapReduce is slow if data is changing frequently</li>\n<li>MapReduce does not return any results until the entire process is finished</li>\n<li>MapReduce hides complexities of parallel programming and greatly simplifies building parallel applications</li>\n<li>MapReduce consists of three main steps: <strong>Mapping</strong>, <strong>Shuffling</strong> and <strong>Reducing</strong></li>\n<li><strong>CASE STUDY:</strong> <a href=\"https://words.sdsc.edu/words-data-science/mapreduce\">https://words.sdsc.edu/words-data-science/mapreduce</a> [Definitely Worth Reading - 5 mins]</li>\n</ul>\n<h3>Hadoop</h3>\n<ul>\n<li>\n<p>When to use Hadoop:</p>\n<ul>\n<li>Large scale growth of data</li>\n<li>Quick access to old data</li>\n<li>Multiple applications over the same data store</li>\n<li>High volume or High variety of data</li>\n<li>Data parallelism</li>\n</ul>\n</li>\n<li>\n<p>When not to use Hadoop:</p>\n<ul>\n<li>Small Datasets</li>\n<li>Advanced Algorithms</li>\n<li>Infrastructure replacement</li>\n<li>Task level parallelism</li>\n<li>Random data access</li>\n</ul>\n</li>\n</ul>\n<br/>\n<hr/>\n<p><a href=\"https://www.coursera.org/account/accomplishments/certificate/WTX9MC52Q8E3\">Course Completion Certificate: Introduction to Big Data</a></p>","frontmatter":{"date":"May 29, 2020","title":"Intro to Big Data"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/notes/Coursera: Big Data/Intro To Big Data/"}}}